  import org.apache.spark.sql.{SparkSession, DataFrame}
  import Constants.Constants._
  import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}
  import org.apache.kafka.common.serialization.StringSerializer

  object producer{

    def main(args:Array[String]): Unit ={

      val spark = SparkSession.builder
        .master("local[*]")
        .appName("producer")
        .getOrCreate()

      val df = spark.read.format(DF_FORMAT)
        .option("delimiter", ",")
        .load(SOURCE_DATASET_PATH)
      val colum_names = ColumnNames
      val dfWithHeader = df.toDF(colum_names:_*)
      dfWithHeader.show(20, false)

      val dfnew = dfWithHeader
      dfnew.show(8)

      val producerProps = new java.util.Properties()
      producerProps.put("bootstrap.servers", BOOTSTRAP_SERVER)
      producerProps.put("key.serializer", classOf[StringSerializer].getName)
      producerProps.put("value.serializer", classOf[StringSerializer].getName)


      dfnew.foreachPartition { partition =>
       val partitionProducer = new KafkaProducer[String, String](producerProps)
        partition.foreach { row =>
          val record = new ProducerRecord[String, String](TOPIC,row.mkString(";"))
         partitionProducer.send(record)
        }
        partitionProducer.close()
      }


    }


  }
